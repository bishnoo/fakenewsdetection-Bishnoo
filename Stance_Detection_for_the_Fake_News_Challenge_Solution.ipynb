{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stance_Detection_for_the_Fake_News_Challenge_Solution.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "QI9jhXKPCFcJ"
      },
      "cell_type": "markdown",
      "source": [
        "# Stance Detection for the Fake News Challenge\n",
        "\n",
        "## Identifying Textual Relationships with Deep Neural Nets\n",
        "\n",
        "### Check the problem context [here](https://drive.google.com/open?id=1KfWaZyQdGBw8AUTacJ2yY86Yxgw2Xwq0).\n",
        "\n",
        "### Download files required for the project from [here](https://drive.google.com/open?id=10yf39ifEwVihw4xeJJR60oeFBY30Y5J8)."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "vSNgdEMpenpE"
      },
      "cell_type": "markdown",
      "source": [
        "## Step1: Load the given dataset <h1> [10 marks] </h1>\n",
        "\n",
        "1. Mount the google drive\n",
        "\n",
        "2. Import Glove embeddings\n",
        "\n",
        "3. Import the test and train datasets"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "aPOZRohMiSpQ"
      },
      "cell_type": "markdown",
      "source": [
        "### Mount the google drive to access required project files\n",
        "\n",
        "Run the below commands"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7AS39z1XgFpT",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "S_7yCFdzgFsH",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "bhZdJ4zpwWzN"
      },
      "cell_type": "markdown",
      "source": [
        "#### Path for Project files on google drive\n",
        "\n",
        "**Note:** You need to change this path according where you have kept the files in google drive. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Aol97RUogFuS",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "project_path = \"/content/drive/My Drive/Datasets/Fake News Challenge/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "2ly0VxAnwJ2f"
      },
      "cell_type": "markdown",
      "source": [
        "### Loading the Glove Embeddings"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "xmsPn6PF-cgL",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "with ZipFile(project_path+'glove.6B.zip', 'r') as z:\n",
        "  z.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "TjLJEQ_PwcGi"
      },
      "cell_type": "markdown",
      "source": [
        "### Load the dataset\n",
        "\n",
        "1. Using [read_csv()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) in pandas load the given train datasets files **`train_bodies.csv`** and **`train_stances.csv`**\n",
        "\n",
        "2. Using [merge](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html) command in pandas merge the two datasets based on the Body ID. \n",
        "\n",
        "Note: Save the final merged dataset in a dataframe with name **`dataset`**."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7gXO1WZ-gFwm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_bodies = pd.read_csv(project_path+'train_bodies.csv')\n",
        "train_stances = pd.read_csv(project_path+'train_stances.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "kosAWskdOOT8",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset = pd.merge(train_bodies,train_stances, how='outer', on=['Body ID', 'Body ID'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "g4ycQbBCg20S"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "<h2> Check1:</h2>\n",
        "  \n",
        "<h3> You should see the below output if you run `dataset.head()` command as given below </h3>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "IUtF7iOmj11k",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "tjzVz2ifijmj"
      },
      "cell_type": "markdown",
      "source": [
        "## Step2: Data Pre-processing and setting some hyper parameters needed for model\n",
        "\n",
        "\n",
        "#### Run the code given below to set the required parameters.\n",
        "\n",
        "1. `MAX_SENTS` = Maximum no.of sentences to consider in an article.\n",
        "\n",
        "2. `MAX_SENT_LENGTH` = Maximum no.of words to consider in a sentence.\n",
        "\n",
        "3. `MAX_NB_WORDS` = Maximum no.of words in the total vocabualry.\n",
        "\n",
        "4. `MAX_SENTS_HEADING` = Maximum no.of sentences to consider in a heading of an article."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "KDXSdpvqjuqw",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "MAX_NB_WORDS = 20000\n",
        "MAX_SENTS = 20\n",
        "MAX_SENTS_HEADING = 1\n",
        "MAX_SENT_LENGTH = 20\n",
        "VALIDATION_SPLIT = 0.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "zwE7CPHdiDT-"
      },
      "cell_type": "markdown",
      "source": [
        "### Download the `Punkt` from nltk using the commands given below. This is for sentence tokenization.\n",
        "\n",
        "For more info on how to use it, read [this](https://stackoverflow.com/questions/35275001/use-of-punktsentencetokenizer-in-nltk).\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "lsiKmyJUZ-hU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Gqwm_GbwwnhX"
      },
      "cell_type": "markdown",
      "source": [
        "### Tokenizing the text and loading the pre-trained Glove word embeddings for each token <h1> [20 marks] </h1>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "WfZLR24mm32k"
      },
      "cell_type": "markdown",
      "source": [
        "Keras provides [Tokenizer API](https://keras.io/preprocessing/text/) for preparing text. Read it before going any further."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "fLSn9S-5oG4Z"
      },
      "cell_type": "markdown",
      "source": [
        "#### Import the Tokenizer from keras preprocessing text"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "S-VUgh2yoMlR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "eml0Lge4oOuh"
      },
      "cell_type": "markdown",
      "source": [
        "#### Initialize the Tokenizer class with maximum vocabulary count as `MAX_NB_WORDS` initialized at the start of step2. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Qm85qirPofc2",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "keras.preprocessing.text.Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ', char_level=False, oov_token=None, document_count=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "HBe1KuXDosJ7"
      },
      "cell_type": "markdown",
      "source": [
        "#### Now, using fit_on_texts() from Tokenizer class, lets encode the data \n",
        "\n",
        "Note: We need to fit articleBody and Headline also to cover all the words."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Q5rk-UyBlmyA",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "t = Tokenizer(num_words=MAX_NB_WORDS) \n",
        "t.fit_on_texts(dataset['articleBody'])\n",
        "t.fit_on_texts(dataset['Headline'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "omptHX-JpBsN"
      },
      "cell_type": "markdown",
      "source": [
        "#### fit_on_texts() gives the following attributes in the output as given [here](https://faroit.github.io/keras-docs/1.2.2/preprocessing/text/).\n",
        "\n",
        "* **word_counts:** dictionary mapping words (str) to the number of times they appeared on during fit. Only set after fit_on_texts was called.\n",
        "\n",
        "* **word_docs:** dictionary mapping words (str) to the number of documents/texts they appeared on during fit. Only set after fit_on_texts was called.\n",
        "\n",
        "* **word_index:** dictionary mapping words (str) to their rank/index (int). Only set after fit_on_texts was called.\n",
        "\n",
        "* **document_count:** int. Number of documents (texts/sequences) the tokenizer was trained on. Only set after fit_on_texts or fit_on_sequences was called.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "SHnsT2sTtFAA"
      },
      "cell_type": "markdown",
      "source": [
        "### Now, tokenize the sentences using nltk sent_tokenize() and encode the senteces with the ids we got form the above `t.word_index`\n",
        "\n",
        "Initialise 2 lists with names `texts` and `articles`.\n",
        "\n",
        "```\n",
        "texts = [] to store text of article as it is.\n",
        "\n",
        "articles = [] split the above text into a list of sentences.\n",
        "```"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ctEu-d4c4EZs",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "texts = dataset['articleBody']\n",
        "\n",
        "articles = [sent_tokenize(i) for i in texts]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "koTVJjoO6P78"
      },
      "cell_type": "markdown",
      "source": [
        "## Check 2:\n",
        "\n",
        "first element of texts and articles should be as given below. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "3mWBW99p5UW9",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "texts[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "WtIjO3ht5EKA",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "articles[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "fpuRIA7cCfcY"
      },
      "cell_type": "markdown",
      "source": [
        "#### Now iterate through each article and each sentence to encode the words into ids using t.word_index <h1>[20 marks]</h1>\n",
        "\n",
        "Here, to get words from sentence you can use `text_to_word_sequence` from keras preprocessing text.\n",
        "\n",
        "1. Import text_to_word_sequence\n",
        "\n",
        "2. Initialize a variable of shape (no.of articles, MAX_SENTS, MAX_SENT_LENGTH) with name `data` with zeros first (you can use numpy [np.zeros](https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html) to initialize with all zeros)and then update it while iterating through the words and sentences in each article."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "YVyClBULCqWj",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "import numpy as np\n",
        "\n",
        "data = np.zeros((len(articles),MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QNRwYJ5Cn3dZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i, article in enumerate(articles):\n",
        "    \n",
        "    for j, text in enumerate(article):\n",
        "       \n",
        "        if j < MAX_SENTS:\n",
        "                wordTokens = text_to_word_sequence(text)\n",
        "                k = 0\n",
        "                for _, word in enumerate(wordTokens):\n",
        "                    if(k < MAX_SENT_LENGTH and t.word_index[word] < MAX_NB_WORDS):\n",
        "                        data[i,j,k] = t.word_index[word]\n",
        "                        k = k + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "bFdmiDYcE144"
      },
      "cell_type": "markdown",
      "source": [
        "### Check 3:\n",
        "\n",
        "Accessing first element in data should give something like given below."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "TsFWW5C2Djog",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data[0, :, :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "hTG6JySHehkT"
      },
      "cell_type": "markdown",
      "source": [
        "### Repeat the same process for the `Headings` as well. Use variables with names `texts_heading` and `articles_heading` accordingly. <h1> [10 marks] </h1>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "_CliiIhLemJV",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "texts_heading = dataset['Headline']\n",
        "articles_heading = [sent_tokenize(i) for i in texts_heading]\n",
        "\n",
        "#print(texts_heading[0])\n",
        "#print(articles_heading[0])\n",
        "\n",
        "data_heading = np.zeros((len(articles_heading),MAX_SENTS_HEADING, MAX_SENT_LENGTH), dtype='int32')\n",
        "\n",
        "#data_heading[0, :, :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W8Wa606F1EcG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i, article in enumerate(articles_heading):\n",
        "    for j, text in enumerate(article):\n",
        "        if j < MAX_SENTS_HEADING:\n",
        "                wordTokens = text_to_word_sequence(text)\n",
        "                k = 0\n",
        "                for _, word in enumerate(wordTokens):\n",
        "                    if(k < MAX_SENT_LENGTH and t.word_index[word] < MAX_NB_WORDS):\n",
        "                        data_heading[i,j,k] = t.word_index[word]\n",
        "                        k = k + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "iaH0Ey1qe_Co"
      },
      "cell_type": "markdown",
      "source": [
        "### Now the features are ready, lets make the labels ready for the model to process.\n",
        "\n",
        "### Convert labels into one-hot vectors\n",
        "\n",
        "You can use [get_dummies](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) in pandas to create one-hot vectors."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Zq-VcgM8fat1",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "labels = pandas.get_dummies(dataset['Stance'], prefix=None, prefix_sep='_', dummy_na=False, columns=None, sparse=False, drop_first=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "40mA8FI2fcxZ"
      },
      "cell_type": "markdown",
      "source": [
        "### Check 4:\n",
        "\n",
        "The shape of data and labels shoould match the given below numbers."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "vpEWEnjFfnFR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "sDOxHdR3frDu"
      },
      "cell_type": "markdown",
      "source": [
        "### Shuffle the data"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-Ra-yYTvfzRt",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## get numbers upto no.of articles\n",
        "indices = np.arange(data.shape[0])\n",
        "## shuffle the numbers\n",
        "np.random.shuffle(indices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "LKnSqwIFf3Iy",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## shuffle the data\n",
        "data = data[indices]\n",
        "data_heading = data_heading[indices]\n",
        "## shuffle the labels according to data\n",
        "labels = labels[indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "JcOFVfPBf9kA"
      },
      "cell_type": "markdown",
      "source": [
        "### Split into train and validation sets. Split the train set 80:20 ratio to get the train and validation sets.\n",
        "\n",
        "\n",
        "Use the variable names as given below:\n",
        "\n",
        "x_train, x_val - for body of articles.\n",
        "\n",
        "x-heading_train, x_heading_val - for heading of articles.\n",
        "\n",
        "y_train - for training labels.\n",
        "\n",
        "y_val - for validation labels.\n",
        "\n",
        "<h1> [10 marks] </h1>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2neh9Wcof8iR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "o5u3PTz3gEV-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train = data[:-num_validation_samples]\n",
        "x_val = data[-num_validation_samples:]\n",
        "x_heading_train = data_heading[:-num_validation_samples]\n",
        "x_heading_val = data_heading[-num_validation_samples:]\n",
        "y_train = labels[:-num_validation_samples]\n",
        "y_val = labels[-num_validation_samples:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "UTyvoHrsgMDw"
      },
      "cell_type": "markdown",
      "source": [
        "### Check 5:\n",
        "\n",
        "The shape of x_train, x_val, y_train and y_val should match the below numbers."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "KLEbiw2Yghe2",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "yNnoBtArhJ1E"
      },
      "cell_type": "markdown",
      "source": [
        "### Create embedding matrix with the glove embeddings\n",
        "\n",
        "\n",
        "Run the below code to create embedding_matrix which has all the words and their glove embedding if present in glove word list."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "eKqn2IL2ZF8v",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocab_size = 27873\n",
        "# load the whole embedding into memory\n",
        "embeddings_index = dict()\n",
        "f = open('./glove.6B.100d.txt')\n",
        "\n",
        "for line in f:\n",
        "\tvalues = line.split()\n",
        "\tword = values[0]\n",
        "\tcoefs = np.asarray(values[1:], dtype='float32')\n",
        "\tembeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "\n",
        "\n",
        "for word, i in t.word_index.items():\n",
        "\tembedding_vector = embeddings_index.get(word)\n",
        "\tif embedding_vector is not None:\n",
        "\t\tembedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "LRi4o3ZspDFU"
      },
      "cell_type": "markdown",
      "source": [
        "## Try a bidirectional LSTM model and report the accuracy score\n",
        "\n",
        "<h1>[20 marks]  </h1>"
      ]
    },
    {
      "metadata": {
        "id": "7-lG9jRQueE3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train = x_train.reshape(-1,4)\n",
        "x_val = x_val.reshape(-1,4)\n",
        "\n",
        "x_train = x_train[:39978,:4]\n",
        "x_val = x_val[:9994,:4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "zSZDnPWkw2ZZ"
      },
      "cell_type": "markdown",
      "source": [
        "### Import layers from Keras to build the model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5AgwQsfMrzAQ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Embedding\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Input, Flatten\n",
        "from keras.layers import Embedding, Dropout, LSTM, GRU, Bidirectional\n",
        "from keras.models import Model\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "gpkVhIbx3gr1"
      },
      "cell_type": "markdown",
      "source": [
        "### Model"
      ]
    },
    {
      "metadata": {
        "id": "cal9qhn4fT4t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 100\n",
        "LSTM_DIM = 100\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(t.word_index),\n",
        "                          output_dim=EMBEDDING_DIM,\n",
        "                          weights = [embedding_matrix], trainable=True, name='word_embedding_layer', #False\n",
        "                          mask_zero=True))\n",
        "model.add(Bidirectional(LSTM(LSTM_DIM, return_sequences=False, name='Bidrectional_lstm_layer1')))\n",
        "model.add(Dropout(rate=0.8, name='dropout_1')) # Can try varying dropout rates, in paper suggest 0.8\n",
        "model.add(Dense(4, activation='softmax', name='output_layer'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "C5Xrd-JQ3id7"
      },
      "cell_type": "markdown",
      "source": [
        "### Compile and fit the model"
      ]
    },
    {
      "metadata": {
        "id": "xB4HbhwN7kyl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w2DGc3J57k4T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"model fitting - Bidirectional LSTM\")\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "w_btKAc6tNkR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#model.fit(np.array(x1_train), np.array(y_train),epochs=10,batch_size=32,validation_data=(np.array(x1_val), np.array(y_val)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6RfeWhFZiOXv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "N_EPOCHS = 40 \n",
        "model.fit(np.array(x_train), np.array(y_train),\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=N_EPOCHS,\n",
        "          validation_data=(np.array(x_val), np.array(y_val)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "A-2HrLs0tPKj"
      },
      "cell_type": "markdown",
      "source": [
        "### Add Attention layer in the LSTM model to impove the accuracy score (Optional)"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "_PsIwWwhtXwB",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
        "from keras.models import Model\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer, InputSpec\n",
        "from keras import initializers\n",
        "EMBEDDING_DIM = 100\n",
        "embedding_layer = Embedding(len(t.word_index),\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SENT_LENGTH,\n",
        "                            trainable=True,\n",
        "                            mask_zero=True)\n",
        "\n",
        "\n",
        "class AttLayer(Layer):\n",
        "    def __init__(self, attention_dim):\n",
        "        self.init = initializers.get('normal')\n",
        "        self.supports_masking = True\n",
        "        self.attention_dim = attention_dim\n",
        "        super(AttLayer, self).__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
        "        self.b = K.variable(self.init((self.attention_dim, )))\n",
        "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
        "        self.trainable_weights = [self.W, self.b, self.u]\n",
        "        super(AttLayer, self).build(input_shape)\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return mask\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        # size of x :[batch_size, sel_len, attention_dim]\n",
        "        # size of u :[batch_size, attention_dim]\n",
        "        # uit = tanh(xW+b)\n",
        "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
        "        ait = K.dot(uit, self.u)\n",
        "        ait = K.squeeze(ait, -1)\n",
        "\n",
        "        ait = K.exp(ait)\n",
        "\n",
        "        if mask is not None:\n",
        "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
        "            ait *= K.cast(mask, K.floatx())\n",
        "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "        ait = K.expand_dims(ait)\n",
        "        weighted_input = x * ait\n",
        "        output = K.sum(weighted_input, axis=1)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[-1])\n",
        "\n",
        "\n",
        "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
        "embedded_sequences = embedding_layer(sentence_input)\n",
        "l_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
        "l_att = AttLayer(100)(l_lstm)\n",
        "sentEncoder = Model(sentence_input, l_att)\n",
        "\n",
        "review_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
        "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
        "l_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(review_encoder)\n",
        "l_att_sent = AttLayer(100)(l_lstm_sent)\n",
        "preds = Dense(4, activation='softmax')(l_att_sent)\n",
        "model = Model(review_input, preds)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
        "\n",
        "print(\"model fitting - Hierachical attention network\")\n",
        "#model.fit(np.array(x_train), np.array(y_train), validation_data=(np.array(x_val), np.array(y_val),epoch=10, batch_size=50)\n",
        "BATCH_SIZE = 128\n",
        "N_EPOCHS = 40 \n",
        "model.fit(np.array(x_train), np.array(y_train),\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=N_EPOCHS,\n",
        "          validation_data=(np.array(x_val), np.array(y_val)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}